{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rl_trader import MultiStockEnv, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  # 0 = AAPL\n",
    "  # 1 = MSI\n",
    "  # 2 = SBUX\n",
    "  df = pd.read_csv('aapl_msi_sbux.csv')\n",
    "  return df.values\n",
    "\n",
    "\n",
    "def get_scaler(env):\n",
    "  # return scikit-learn scaler object to scale the states\n",
    "  # Note: you could also populate the replay buffer here\n",
    "\n",
    "  states = []\n",
    "  for _ in range(env.n_step):\n",
    "    action = np.random.choice(env.action_space)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    states.append(state)\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(states)\n",
    "  return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, n_action, n_hidden_layers=1, hidden_dim=32):\n",
    "    \"\"\" A multi-layer perceptron \"\"\"\n",
    "\n",
    "    # input layer\n",
    "    i = Input(shape=(input_dim,))\n",
    "    x = i\n",
    "\n",
    "    # hidden layers\n",
    "    for _ in range(n_hidden_layers):\n",
    "        x = Dense(hidden_dim, activation='relu')(x)\n",
    "\n",
    "    # final layer\n",
    "    x = Dense(n_action)(x)\n",
    "\n",
    "    # make the model\n",
    "    model = Model(i, x)\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    print((model.summary()))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = build_model(state_size, action_size)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "    \n",
    "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    \n",
    "    def replay(self, batch_size=32):\n",
    "        # first check if replay buffer contains enough data\n",
    "        if self.memory.size < batch_size:\n",
    "            return\n",
    "\n",
    "        # sample a batch of data from the replay memory\n",
    "        minibatch = self.memory.sample_batch(batch_size)\n",
    "        states = minibatch['s']\n",
    "        actions = minibatch['a']\n",
    "        rewards = minibatch['r']\n",
    "        next_states = minibatch['s2']\n",
    "        done = minibatch['d']\n",
    "\n",
    "        # Calculate the tentative target: Q(s',a)\n",
    "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "\n",
    "        # With the Keras API, the target (usually) must have the same\n",
    "        # shape as the predictions.\n",
    "        # However, we only need to update the network for the actions\n",
    "        # which were actually taken.\n",
    "        # We can accomplish this by setting the target to be equal to\n",
    "        # the prediction for all values.\n",
    "        # Then, only change the targets for the actions taken.\n",
    "        # Q(s,a)\n",
    "        target_full = self.model.predict(states)\n",
    "        target_full[np.arange(batch_size), actions] = target\n",
    "\n",
    "        # Run one training step\n",
    "        self.model.train_on_batch(states, target_full)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(agent, env, is_train, scaler):\n",
    "  # note: after transforming states are already 1xD\n",
    "  state = env.reset()\n",
    "  state = scaler.transform([state])\n",
    "  done = False\n",
    "\n",
    "  while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = scaler.transform([next_state])\n",
    "    if is_train == 'train':\n",
    "      agent.update_replay_memory(state, action, reward, next_state, done)\n",
    "      agent.replay(batch_size)\n",
    "    state = next_state\n",
    "\n",
    "  return info['cur_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    NUM_EPISODE = 3\n",
    "    IS_TRAIN = True\n",
    "    \n",
    "    data = get_data()\n",
    "    n_timestamp, n_stocks = data.shape\n",
    "    n_train = n_timestamp // 2  # divide and round\n",
    "\n",
    "    train_data = data[:n_train]\n",
    "    test_data = data[n_train:]   \n",
    "\n",
    "    env = MultiStockEnv(get_data())\n",
    "    env.reset()\n",
    "    state_size = env.state_dim\n",
    "    action_size = len(env.action_space)\n",
    "\n",
    "    scaler = get_scaler(env)\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    # store the final value of the portfolio (end of episode)\n",
    "    portfolio_value = []\n",
    "    \n",
    "      # play the game num_episodes times\n",
    "    for e in range(NUM_EPISODE):\n",
    "        t0 = datetime.now()\n",
    "        val = play_one_episode(agent, env, IS_TRAIN, scaler)\n",
    "        current_time = datetime.now() - t0\n",
    "        print(f\"episode: {e + 1}/{NUM_EPISODE}, episode end value: {val:.2f}, duration: {current_time}\")\n",
    "        portfolio_value.append(val) # append episode end portfolio value\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
